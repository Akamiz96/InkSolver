"""
===============================================================================
Proyecto: Inksolver
Archivo: validate_test_classification.py
Descripcion: Valida el proceso de clasificacion de operadores matematicos en pruebas utilizando una matriz de confusion.
Autor: Alejandro Castro Martinez
Fecha de creacion: 2025-03-17
Ultima modificacion: 2025-03-18
Version: 1.0
===============================================================================
Dependencias:
- Python 3.10
- Librerias externas: Pandas, NumPy, Matplotlib, Seaborn, os, warnings, scikit-learn
===============================================================================
Uso:
Ejecutar el script con el siguiente comando:
    python validate_test_classification.py
===============================================================================
Notas:
- El dataset de clasificacion de prueba debe estar en 'test_results/classified_test_images.csv'.
- Los resultados de validacion se guardaran en 'test_validation/'.
===============================================================================
"""

import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from sklearn.metrics import confusion_matrix

# Suprimir warnings innecesarios
warnings.simplefilter("ignore", category=UserWarning)

# Definir rutas de entrada y salida
test_csv_path = "test_results/classified_test_images.csv"  # Archivo de clasificaciÃ³n de prueba
output_folder = "test_validation"  # Carpeta para guardar los resultados
os.makedirs(output_folder, exist_ok=True)

# Definir rutas de salida para los archivos generados
conf_matrix_img_path = os.path.join(output_folder, "confusion_matrix.png")
conf_matrix_csv_path = os.path.join(output_folder, "confusion_matrix.csv")
report_txt_path = os.path.join(output_folder, "classification_report.txt")

# Cargar el archivo CSV con los resultados de la clasificaciÃ³n de prueba
print("\n\033[91mğŸ”´ Cargando datos desde el archivo de clasificaciÃ³n de prueba...\033[0m")
df = pd.read_csv(test_csv_path)

# Extraer etiquetas reales y predichas del dataset
print("\033[91mğŸ”´ Extrayendo etiquetas reales y predichas...\033[0m")
y_true = df["Operador_Real"]  # Etiquetas reales
y_pred = df["Prediccion"]  # Etiquetas predichas

# Obtener clases Ãºnicas, asegurando incluir la categorÃ­a "Desconocido"
print("\033[91mğŸ”´ Definiendo clases y asegurando la presencia de 'Desconocido'...\033[0m")
classes = sorted(set(y_true) | set(y_pred) | {"Desconocido"})

# ConstrucciÃ³n de la matriz de confusiÃ³n
print("\033[91mğŸ”´ Generando matriz de confusiÃ³n...\033[0m")
conf_matrix = confusion_matrix(y_true, y_pred, labels=classes)

# Guardar la matriz de confusiÃ³n en un archivo CSV
print("\033[91mğŸ”´ Guardando la matriz de confusiÃ³n en CSV...\033[0m")
conf_matrix_df = pd.DataFrame(conf_matrix, index=classes, columns=classes)
conf_matrix_df.to_csv(conf_matrix_csv_path)

# Graficar la matriz de confusiÃ³n
print("\033[91mğŸ”´ Generando y guardando la imagen de la matriz de confusiÃ³n...\033[0m")
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel("PredicciÃ³n")
plt.ylabel("Etiqueta Real")
plt.title("Matriz de ConfusiÃ³n")
plt.savefig(conf_matrix_img_path, dpi=300, bbox_inches="tight")

# Intentar mostrar la imagen sin errores
try:
    plt.show(block=False)
except:
    pass

# Calcular la precisiÃ³n global del modelo
print("\033[91mğŸ”´ Calculando precisiÃ³n global del modelo...\033[0m")
total_correct_predictions = np.sum(np.diag(conf_matrix))
total_predictions = np.sum(conf_matrix)
accuracy_percentage = (total_correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0.0

# Crear un diccionario para almacenar mÃ©tricas por clase
metrics_per_class = {}

# Calcular mÃ©tricas clave: PrecisiÃ³n, Recall, Especificidad y F1-Score
print("\033[91mğŸ”´ Calculando mÃ©tricas por clase...\033[0m")
for i, class_label in enumerate(classes):
    TP = conf_matrix[i, i]  # Verdaderos Positivos
    FN = np.sum(conf_matrix[i, :]) - TP  # Falsos Negativos
    FP = np.sum(conf_matrix[:, i]) - TP  # Falsos Positivos
    TN = total_predictions - (TP + FN + FP)  # Verdaderos Negativos

    precision = (TP / (TP + FP)) * 100 if (TP + FP) > 0 else 0.0
    recall = (TP / (TP + FN)) * 100 if (TP + FN) > 0 else 0.0
    specificity = (TN / (TN + FP)) * 100 if (TN + FP) > 0 else 0.0
    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

    metrics_per_class[class_label] = {
        "PrecisiÃ³n (%)": precision,
        "Recall (%)": recall,
        "Especificidad (%)": specificity,
        "F1-Score (%)": f1_score
    }

# Generar el reporte de validaciÃ³n
print("\033[91mğŸ”´ Generando reporte de validaciÃ³n...\033[0m")
with open(report_txt_path, "w", encoding="utf-8") as f:
    f.write("ğŸ“Š VALIDACIÃ“N DE CLASIFICACIÃ“N DE TEST ğŸ“Š\n\n")
    f.write(f"âœ… PrecisiÃ³n Global del Modelo: {accuracy_percentage:.2f}%\n\n")
    for class_label, metrics in metrics_per_class.items():
        f.write(f"ğŸ”¹ Clase '{class_label}':\n")
        f.write(f"   ğŸ¯ PrecisiÃ³n: {metrics['PrecisiÃ³n (%)']:.2f}%\n")
        f.write(f"   ğŸ” Recall: {metrics['Recall (%)']:.2f}%\n")
        f.write(f"   ğŸš€ Especificidad: {metrics['Especificidad (%)']:.2f}%\n")
        f.write(f"   âš–ï¸ F1-Score: {metrics['F1-Score (%)']:.2f}%\n\n")

# Mensaje final de validaciÃ³n
print("\n\033[92mâœ… VALIDACIÃ“N COMPLETADA: RESULTADOS GUARDADOS\033[0m")
print(f"\033[93mğŸ“‚ Matriz de ConfusiÃ³n guardada en: {conf_matrix_csv_path}\033[0m")
print(f"\033[93mğŸ“‚ Imagen de Matriz de ConfusiÃ³n guardada en: {conf_matrix_img_path}\033[0m")
print(f"\033[93mğŸ“‚ Reporte de ValidaciÃ³n guardado en: {report_txt_path}\033[0m")
